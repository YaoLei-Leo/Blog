[{"content":"Taylor Expansion / Series\rIn mathematics, the Taylor expansion / series of a function is an infinite sum of terms that are expressed in terms of the function\u0026rsquo;s derivatives at a single point. For most common functions, the function and the sum of its Taylor series are equal near this point.\nSuppose $f(x)$ is a real or composite function, which is a differentiable function of a neighborhood of the point $a$. The Taylor series of $f(x)$ about the point $a$ is given by:\n$$f(x)=f(a)+\\frac{f^{\\prime}(a)}{1!}(x-a)+\\frac{f^{\\prime\\prime}(a)}{2!}(x-a)^2+\\frac{f^{\\prime\\prime\\prime}(a)}{3!}(x-a)^3+\\dots$$\nWhere $n!$ denotes the factorial of $n$. In the more compact sigma notation, this can be written as: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n$$\nExample: the power series for $e^x$ is: $$e^x=\\sum_{n=0}^\\infty \\frac{x^n}{n!}$$\nProof.\nAs the derivative of $e^x$ is itself: $$f^{(n)}(x)=e^x$$\nTaking the derivative at $x=0$: $$f^{(n)}(0)=e^0=1$$\nUsing Taylor expansion, set $a=0$: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n=\\sum_{n=0}^\\infin\\frac{x^n}{n!}$$\nIn summary\nTaylor expansion is a way to approximate a function using infinitely many polynomial terms.\nPower Series\rA power series is an infinite series of the form: $$\\sum_{n=0}^\\infty a_n(x-a)^n$$\nWhere:\n$a_n$ are the coefficients of the series $x$ is the variable $a$ is a constant. Example: the power series for $sin(x)$ is: $$sin(x)=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+\\frac{x^9}{9!}-\\frac{x^11}{11!}+\\dots$$\nProof.\nUsing the Taylor expansion with $a=0$: $$f(x)=sin(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n$$\nCompute the derivatives of $sin(x)$ at $x=0$: $$f(0)=sin(0)=0$$ $$f^{(1)}(0)=cos(0)=1$$ $$f^{(2)}(0)=-sin(0)=0$$ $$f^{(3)}(0)=-cos(0)=-1$$ $$f^{(4)}(0)=sin(0)=0$$ $$f^{(5)}(0)=cos(0)=1$$ $$f^{(6)}(0)=-sin(0)=0$$ $$f^{(7)}(0)=-cos(0)=-1$$ $$\\dots$$ You can see that only the odd derivatives are non-zero, and they alternate in sign.\nPlug into the Taylor expansion: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$\nIn Summary\nEvery Taylor series is a power series, but not every power series is a Taylor series. Power series serve as a broad framework, while Taylor series are tailored (pun intended!) to approximate specific functions using their derivatives. The convergence of a Taylor series to its function depends on the function being analytic (i.e., equal to its Taylor series in some neighborhood). ","date":"2025-05-13T20:14:20+08:00","image":"https://example.org/post/taylor_expansion/Figure_hu7592963123002402754.jpg","permalink":"https://example.org/post/taylor_expansion/","title":"Taylor Expansion / Series and Power Series"},{"content":"Moments in Probability\rFor a random variable $X$, The $n$-th moments of $X$ is the expectation of $X^n$, which is $\\mathbb{E}[X^n]$.\nWhy we need moment? Let’s starts with some measures we often use to describe a distribution.\nMean: $\\mu=\\mathbb{E}[X]$. Mean measures the central tendency. It tells us something about the center of a distribution. It is the $1$-th moment of $X$.\nVariance: $\\sigma=\\mathbb{E}[X-\\mu]^2$, Variance measures of dispersion. It is a measure of how far a set of numbers is spread out from their average value. It is the $2$-nd moment of $X-\\mu$.\nSkewness: $Skew(X)=\\mathbb{E}[\\frac{X-\\mu}{\\sigma}]^3$. Skewness measures the level of asymmetry level of a distribution. It is the $3$-rd moment of standardized $X$.\nKurtosis:: $Kurt(X)=\\mathbb{E}[\\frac{X-μ}{σ}]^4-3$. Kurtosis measures the tail’s heavy level of a distribution. It is the $4$-th moment of standardized $X$. The reason for subtracting 3 is to make any Normal distribution have kurtosis of 0.\nAs we can see from the example, moment could be used to measure the characteristics of distributions.\nSample moments\rIn previous section, we used some examples to introduce the moment, and why we need it. We used $\\mathbb{E}[X]$, which is the $1$-st moment of $X$, to represent the mean. Back to primary school, we were taught to use $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nx_i$ to calculate the mean of a observed data. For example, if we observe a list of $[1,3,4,2,2,6]$, the mean of this list would be $(1+3+4+2+2+6)/6=3$. Here is the question, what is the difference between $\\mathbb{E}[X]$ and $\\bar{X}$?\nIn probability, we use $\\mathbb{E}[X]$ to represent the mean of a random variable $X$. In statistics, we use $\\bar{X}$ to represent the mean of a observed data (sampled data, also called sample). $\\mathbb{E}[X]$ is the population mean of a random variable. Let\u0026rsquo;s say $X \\backsim N(0,1)$, then the $\\mathbb{E}[X]$ is equal to $0$. While $\\bar{X}$ is the sample mean of a observed data. In the real situation, we can only observe a sample of $X$, not the whole population. Therefore, the $\\bar{X}$ calculated from $\\frac{1}{n}\\sum_{i=1}^nx_i$ might not eactly equal to $0$. As the sample mean $\\bar{X}$ is an estimate of the population mean $\\mathbb{E}[X]$.\nSimilarly, the sample variance is defined as: $$ S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2 $$ The sample variance $S_n^2$ is an estimate of the population variance $\\sigma^2$. The sample variance is an unbiased estimator of the population variance. The reason for using $n-1$ instead of $n$ is to make the sample variance an unbiased estimator of the population variance. If we use $n$, then the sample variance would be biased.\nSimilarly, we can define the sample skewness as: $$ S_n^3=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^3 $$ and the sample kurtosis as: $$ S_n^4=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^4-3 $$\nMoment generating functions\rTheorem (Moment via derivatives of the MGF). The moment generating function (MGF) of a random variable $X$ is defined as: $$ M_X(t)=\\mathbb{E}[e^{tX}] $$ Given the MGF of $X$, we can get the $n$-th moment of $X$ by taking the $n$-th derivative of the MGF and evaluating it at $t=0$: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nProof.\nUsing Taylor expansion of $M_X(t)$ around $t=0$: $$M_X(t)=\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin M^{(0)}(0)\\frac{t^n}{n!} \\tag{1}$$\nUsing Power series expansion of $e^{tX}$: $$e^{tX}=\\sum_{n=0}^\\infin \\frac{(tX)^n}{n!}=\\sum_{n=0}^\\infin \\frac{t^nX^n}{n!}$$\nSo the expectation of $e^{tX}$ is: $$\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin \\frac{t^n}{n!}\\mathbb{E}[X^n] \\tag{2}$$\nComparing (1) and (2), we can get: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nThis theorem is suprising in that for a continuous random variable $X$, to compute moments would seemingly require doing integrals with LOTUS, but with the MGF, it is possible to find moments by taking derivatives rather than doing integrals.\n","date":"2025-05-13T00:53:25+08:00","image":"https://example.org/post/moments/Minecraft_DH_hu9496421566315252534.png","permalink":"https://example.org/post/moments/","title":"Moments in Probability"}]