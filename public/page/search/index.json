[{"content":"Definition of expectation\rDefinition (Expectation of a discrete random variable)\nThe expected value of a discrete random variable $X$ whose distinct possible values are $x_1, x_2, \\ldots$ is defined by $$\\mathbb E[X] = \\sum_{i} x_i P(X = x_i) = \\sum_{i} x_i f_X(x_i)$$ where $f_X(x)$ is the probability mass function of $X$.\nThe expected value of a continuous random variable $X$ is defined by $$\\mathbb E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx$$ where $f_X(x)$ is the probability density function of $X$.\nLaw of unconscious statistician (LOTUS)\rTheorem (LOTUS)\nIf $X$ is a discrete random variable, and $g$ is a function from $\\mathbb R$ to $\\mathbb R$. If $Y=g(X)$, then $$\\mathbb E[Y] = \\mathbb E[g(X)] = \\sum_{x} g(x) P(X = x) = \\sum_{x} g(x) f_X(x)$$ where $f_X(x)$ is the probability mass function (PMF) of $X$.\nIf $X$ is a continuous random variable, and $g$ is a function from $\\mathbb R$ to $\\mathbb R$. If $Y=g(X)$, then $$\\mathbb E[Y] = \\mathbb E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx$$ where $f_X(x)$ is the probability density function (PDF) of $X$.\nWhy LOTUS?\rExample 1\nLet:\n$X \\sim Exponential(\\lambda)$ $Y=log(X)$ If we use the definition of expectation to calculate $\\mathbb E[Y]$, we can first calculate the PDF of $Y$: $$f_Y(y) = \\frac{d}{dy} P(Y \\leq y) = \\frac{d}{dy} P(X \\leq e^y) = \\frac{d}{dy} (1 - e^{-\\lambda e^y})$$ $$= \\lambda e^{-\\lambda e^y} e^y = \\lambda e^{-\\lambda e^y + y}$$\nThen we can calculate $\\mathbb E[Y]$: $$\\mathbb E[Y] = \\int_{-\\infty}^{\\infty} y f_Y(y) dy = \\int_{-\\infty}^{\\infty} y \\lambda e^{-\\lambda e^y + y} dy$$\nHowever, LOTUS could make it easier: $$\\mathbb E[Y] = \\mathbb E[log(X)] = \\int_{0}^{\\infty} log(x) \\lambda e^{-\\lambda x} dx$$\nExample 2\nLet:\n$X \\sim N(0,1)$ $Y = sin(X)$ If we use the definition of expectation to calculate $\\mathbb E[Y]$, we can first calculate the PDF of $Y$: $$f_Y(y) = \\frac{d}{dy} P(Y \\leq y) = \\frac{d}{dy} P(X \\leq sin^{-1}(y))$$ However, there’s no closed-form PDF for $Y$ as we cannot write the analytic form of $sin^{-1}(y)$. It’s a complicated, oscillatory transformation of a normal variable. You can’t write down $f_Y(y)$ analytically. Consequently, we cannot calculate $\\mathbb E[Y]$ using the definition of expectation.\nHowever, we can use LOTUS: $$\\mathbb E[Y] = \\mathbb E[sin(X)] = \\int_{-\\infty}^{\\infty} sin(x) f_X(x) dx = \\int_{-\\infty}^{\\infty} sin(x) \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} dx$$ This is a analytically solvable integral. The result is $0$ because the integrand is an odd function and the limits are symmetric about $0$.\nIn summary\rLOTUS is a powerful tool for calculating the expected value of a function of a random variable. It allows us to avoid the need to find the PDF of the transformed variable. It can simplify the calculation of expected values for complex transformations. It is applicable to both discrete and continuous random variables. ","date":"2025-05-14T15:10:48+08:00","image":"http://localhost:1313/post/lotus/windmill_hu16029235293494346924.png","permalink":"http://localhost:1313/post/lotus/","title":"LOTUS"},{"content":"Theorem (Change of variables in one dimension) Let $X$ be a random variable with PDF $f_X$, and let $Y=g(X)$, where $g$ is differentiable and monotonic. Then the PDF of $Y$ is given by: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$.\nProof.\nSuppose g is strictly increasing. Then, for $y=g(x)$, the CDF of $Y$ is: $$F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\leq g^-1(y))=F_X(g^-1(y))=F_X(x)$$ Therefore, the PDF of $Y$ is $$f_Y(y)=\\frac{d}{dy}F_Y(y)=\\frac{d}{dy}F_X(x)=\\frac{dx}{dy}\\frac{d}{dx}F_X(x)=f_X(x)\\frac{dx}{dy}$$\nSuppose g is strictly decreasing. Then, for $y=g(x)$, the CDF of $Y$ is: $$F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)=P(X\\geq g^-1(y))=1-P(X\\leq g^-1(y))=1-F_X(g^-1(y))=1-F_X(x)$$ Therefore, the PDF of $Y$ is $$f_Y(y)=\\frac{d}{dy}F_Y(y)=-\\frac{d}{dy}F_X(x)=-\\frac{dx}{dy}\\frac{d}{dx}F_X(x)=-f_X(x)\\frac{dx}{dy}$$ In this case, the $\\frac{dx}{dy}$ is negative, so we can write: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ Combine the two cases: $$f_Y(y)=f_X(x)|\\frac{dx}{dy}|$$ where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$. ","date":"2025-05-14T14:35:45+08:00","image":"http://localhost:1313/House.jpg","permalink":"http://localhost:1313/post/change_of_variables/","title":"Change_of_variables"},{"content":"Taylor Expansion / Series\rIn mathematics, the Taylor expansion / series of a function is an infinite sum of terms that are expressed in terms of the function\u0026rsquo;s derivatives at a single point. For most common functions, the function and the sum of its Taylor series are equal near this point.\nSuppose $f(x)$ is a real or composite function, which is a differentiable function of a neighborhood of the point $a$. The Taylor series of $f(x)$ about the point $a$ is given by:\n$$f(x)=f(a)+\\frac{f^{\\prime}(a)}{1!}(x-a)+\\frac{f^{\\prime\\prime}(a)}{2!}(x-a)^2+\\frac{f^{\\prime\\prime\\prime}(a)}{3!}(x-a)^3+\\dots$$\nWhere $n!$ denotes the factorial of $n$. In the more compact sigma notation, this can be written as: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n$$\nExample: the power series for $e^x$ is: $$e^x=\\sum_{n=0}^\\infty \\frac{x^n}{n!}$$\nProof.\nAs the derivative of $e^x$ is itself: $$f^{(n)}(x)=e^x$$\nTaking the derivative at $x=0$: $$f^{(n)}(0)=e^0=1$$\nUsing Taylor expansion, set $a=0$: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(a)}{n!}(x-a)^n=\\sum_{n=0}^\\infin\\frac{x^n}{n!}$$\nIn summary\nTaylor expansion is a way to approximate a function using infinitely many polynomial terms.\nPower Series\rA power series is an infinite series of the form: $$\\sum_{n=0}^\\infty a_n(x-a)^n$$\nWhere:\n$a_n$ are the coefficients of the series $x$ is the variable $a$ is a constant. Example: the power series for $sin(x)$ is: $$sin(x)=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+\\frac{x^9}{9!}-\\frac{x^11}{11!}+\\dots$$\nProof.\nUsing the Taylor expansion with $a=0$: $$f(x)=sin(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n$$\nCompute the derivatives of $sin(x)$ at $x=0$: $$f(0)=sin(0)=0$$ $$f^{(1)}(0)=cos(0)=1$$ $$f^{(2)}(0)=-sin(0)=0$$ $$f^{(3)}(0)=-cos(0)=-1$$ $$f^{(4)}(0)=sin(0)=0$$ $$f^{(5)}(0)=cos(0)=1$$ $$f^{(6)}(0)=-sin(0)=0$$ $$f^{(7)}(0)=-cos(0)=-1$$ $$\\dots$$ You can see that only the odd derivatives are non-zero, and they alternate in sign.\nPlug into the Taylor expansion: $$f(x)=\\sum_{n=0}^\\infin\\frac{f^{(n)}(0)}{n!}(x)^n=\\sum_{n=0}^\\infin\\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$\nIn Summary\nEvery Taylor series is a power series, but not every power series is a Taylor series. Power series serve as a broad framework, while Taylor series are tailored (pun intended!) to approximate specific functions using their derivatives. The convergence of a Taylor series to its function depends on the function being analytic (i.e., equal to its Taylor series in some neighborhood). ","date":"2025-05-13T20:14:20+08:00","image":"http://localhost:1313/post/taylor_expansion/Figure_hu7592963123002402754.jpg","permalink":"http://localhost:1313/post/taylor_expansion/","title":"Taylor Expansion / Series and Power Series"},{"content":"Moments in Probability\rFor a random variable $X$, The $n$-th moments of $X$ is the expectation of $X^n$, which is $\\mathbb{E}[X^n]$.\nWhy we need moment? Let’s starts with some measures we often use to describe a distribution.\nMean: $\\mu=\\mathbb{E}[X]$. Mean measures the central tendency. It tells us something about the center of a distribution. It is the $1$-th moment of $X$.\nVariance: $\\sigma=\\mathbb{E}[X-\\mu]^2$, Variance measures of dispersion. It is a measure of how far a set of numbers is spread out from their average value. It is the $2$-nd moment of $X-\\mu$.\nSkewness: $Skew(X)=\\mathbb{E}[\\frac{X-\\mu}{\\sigma}]^3$. Skewness measures the level of asymmetry level of a distribution. It is the $3$-rd moment of standardized $X$.\nKurtosis:: $Kurt(X)=\\mathbb{E}[\\frac{X-μ}{σ}]^4-3$. Kurtosis measures the tail’s heavy level of a distribution. It is the $4$-th moment of standardized $X$. The reason for subtracting 3 is to make any Normal distribution have kurtosis of 0.\nAs we can see from the example, moment could be used to measure the characteristics of distributions.\nSample moments\rIn previous section, we used some examples to introduce the moment, and why we need it. We used $\\mathbb{E}[X]$, which is the $1$-st moment of $X$, to represent the mean. Back to primary school, we were taught to use $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nx_i$ to calculate the mean of a observed data. For example, if we observe a list of $[1,3,4,2,2,6]$, the mean of this list would be $(1+3+4+2+2+6)/6=3$. Here is the question, what is the difference between $\\mathbb{E}[X]$ and $\\bar{X}$?\nIn probability, we use $\\mathbb{E}[X]$ to represent the mean of a random variable $X$. In statistics, we use $\\bar{X}$ to represent the mean of a observed data (sampled data, also called sample). $\\mathbb{E}[X]$ is the population mean of a random variable. Let\u0026rsquo;s say $X \\backsim N(0,1)$, then the $\\mathbb{E}[X]$ is equal to $0$. While $\\bar{X}$ is the sample mean of a observed data. In the real situation, we can only observe a sample of $X$, not the whole population. Therefore, the $\\bar{X}$ calculated from $\\frac{1}{n}\\sum_{i=1}^nx_i$ might not eactly equal to $0$. As the sample mean $\\bar{X}$ is an estimate of the population mean $\\mathbb{E}[X]$.\nSimilarly, the sample variance is defined as: $$ S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2 $$ The sample variance $S_n^2$ is an estimate of the population variance $\\sigma^2$. The sample variance is an unbiased estimator of the population variance. The reason for using $n-1$ instead of $n$ is to make the sample variance an unbiased estimator of the population variance. If we use $n$, then the sample variance would be biased.\nSimilarly, we can define the sample skewness as: $$ S_n^3=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^3 $$ and the sample kurtosis as: $$ S_n^4=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_i-\\bar{x}}{S_n})^4-3 $$\nMoment generating functions\rTheorem (Moment via derivatives of the MGF). The moment generating function (MGF) of a random variable $X$ is defined as: $$ M_X(t)=\\mathbb{E}[e^{tX}] $$ Given the MGF of $X$, we can get the $n$-th moment of $X$ by taking the $n$-th derivative of the MGF and evaluating it at $t=0$: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nProof.\nUsing Taylor expansion of $M_X(t)$ around $t=0$: $$M_X(t)=\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin M^{(0)}(0)\\frac{t^n}{n!} \\tag{1}$$\nUsing Power series expansion of $e^{tX}$: $$e^{tX}=\\sum_{n=0}^\\infin \\frac{(tX)^n}{n!}=\\sum_{n=0}^\\infin \\frac{t^nX^n}{n!}$$\nSo the expectation of $e^{tX}$ is: $$\\mathbb{E}[e^{tX}]=\\sum_{n=0}^\\infin \\frac{t^n}{n!}\\mathbb{E}[X^n] \\tag{2}$$\nComparing (1) and (2), we can get: $$\\mathbb{E}[X^n]=M^{(n)}(0)$$\nThis theorem is suprising in that for a continuous random variable $X$, to compute moments would seemingly require doing integrals with LOTUS, but with the MGF, it is possible to find moments by taking derivatives rather than doing integrals.\n","date":"2025-05-13T00:53:25+08:00","image":"http://localhost:1313/post/moments/Minecraft_DH_hu9496421566315252534.png","permalink":"http://localhost:1313/post/moments/","title":"Moments in Probability"}]