<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Moments on Leo&#39;s sharing space</title>
        <link>https://example.org/tags/moments/</link>
        <description>Recent content in Moments on Leo&#39;s sharing space</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 13 May 2025 00:53:25 +0800</lastBuildDate><atom:link href="https://example.org/tags/moments/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Moments in Probability</title>
        <link>https://example.org/post/moments/</link>
        <pubDate>Tue, 13 May 2025 00:53:25 +0800</pubDate>
        
        <guid>https://example.org/post/moments/</guid>
        <description>&lt;img src="https://example.org/post/moments/Minecraft_DH.png" alt="Featured image of post Moments in Probability" /&gt;&lt;h2 id=&#34;moments-in-probability&#34;&gt;Moments in Probability
&lt;/h2&gt;&lt;p&gt;For a random variable $X$, The $n$-th moments of $X$ is the expectation of $X^n$, which is $\mathbb{E}[X^n]$.&lt;/p&gt;
&lt;p&gt;Why we need moment? Let’s starts with some measures we often use to describe a distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mean:&lt;/strong&gt; $\mu=\mathbb{E}[X]$. &lt;em&gt;Mean&lt;/em&gt; measures the central tendency. It tells us something about the center of a distribution. It is the $1$-th moment of $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt; $\sigma=\mathbb{E}[X-\mu]^2$, &lt;em&gt;Variance&lt;/em&gt; measures of dispersion. It is a measure of how far a set of numbers is spread out from their average value. It is the $2$-nd moment of $X-\mu$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skewness:&lt;/strong&gt; $Skew(X)=\mathbb{E}[\frac{X-\mu}{\sigma}]^3$. &lt;em&gt;Skewness&lt;/em&gt; measures the level of asymmetry level of a distribution. It is the $3$-rd moment of standardized $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kurtosis:&lt;/strong&gt;: $Kurt(X)=\mathbb{E}[\frac{X-μ}{σ}]^4-3$. &lt;em&gt;Kurtosis&lt;/em&gt; measures the tail’s heavy level of a distribution. It is the $4$-th moment of standardized $X$. The reason for subtracting 3 is to make any Normal distribution have kurtosis of 0.&lt;/p&gt;
&lt;p&gt;As we can see from the example, moment could be used to measure the characteristics of distributions.&lt;/p&gt;
&lt;h2 id=&#34;sample-moments&#34;&gt;Sample moments
&lt;/h2&gt;&lt;p&gt;In previous section, we used some examples to introduce the moment, and why we need it. We used $\mathbb{E}[X]$, which is the $1$-st moment of $X$, to represent the mean. Back to primary school, we were taught to use $\bar{X}=\frac{1}{n}\sum_{i=1}^nx_i$ to calculate the mean of a observed data. For example, if we observe a list of $[1,3,4,2,2,6]$, the mean of this list would be $(1+3+4+2+2+6)/6=3$. Here is the question, what is the difference between $\mathbb{E}[X]$ and $\bar{X}$?&lt;/p&gt;
&lt;p&gt;In probability, we use $\mathbb{E}[X]$ to represent the mean of a random variable $X$. In statistics, we use $\bar{X}$ to represent the mean of a observed data (sampled data, also called sample). $\mathbb{E}[X]$ is the population mean of a random variable. Let&amp;rsquo;s say $X \backsim N(0,1)$, then the $\mathbb{E}[X]$ is equal to $0$. While $\bar{X}$ is the sample mean of a observed data. In the real situation, we can only observe a sample of $X$, not the whole population. Therefore, the $\bar{X}$ calculated from $\frac{1}{n}\sum_{i=1}^nx_i$ might not eactly equal to $0$. As the sample mean $\bar{X}$ is an estimate of the population mean $\mathbb{E}[X]$.&lt;/p&gt;
&lt;p&gt;Similarly, the sample variance is defined as:
$$ S_n^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2 $$
The sample variance $S_n^2$ is an estimate of the population variance $\sigma^2$. The sample variance is an unbiased estimator of the population variance. The reason for using $n-1$ instead of $n$ is to make the sample variance an unbiased estimator of the population variance. If we use $n$, then the sample variance would be biased.&lt;/p&gt;
&lt;p&gt;Similarly, we can define the &lt;em&gt;sample skewness&lt;/em&gt; as:
$$ S_n^3=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar{x}}{S_n})^3 $$
and the &lt;em&gt;sample kurtosis&lt;/em&gt; as:
$$ S_n^4=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar{x}}{S_n})^4-3 $$&lt;/p&gt;
&lt;h2 id=&#34;moment-generating-functions&#34;&gt;Moment generating functions
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Moment via derivatives of the MGF). The moment generating function (MGF) of a random variable $X$ is defined as:
$$ M_X(t)=\mathbb{E}[e^{tX}] $$
Given the MGF of $X$, we can get the $n$-th moment of $X$ by taking the $n$-th derivative of the MGF and evaluating it at $t=0$: $$\mathbb{E}[X^n]=M^{(n)}(0)$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Using Taylor expansion of $M_X(t)$ around $t=0$:
$$M_X(t)=\mathbb{E}[e^{tX}]=\sum_{n=0}^\infin M^{(0)}(0)\frac{t^n}{n!} \tag{1}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using Power series expansion of $e^{tX}$:
$$e^{tX}=\sum_{n=0}^\infin \frac{(tX)^n}{n!}=\sum_{n=0}^\infin \frac{t^nX^n}{n!}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So the expectation of $e^{tX}$ is:
$$\mathbb{E}[e^{tX}]=\sum_{n=0}^\infin \frac{t^n}{n!}\mathbb{E}[X^n] \tag{2}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Comparing (1) and (2), we can get:
$$\mathbb{E}[X^n]=M^{(n)}(0)$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This theorem is suprising in that for a continuous random variable $X$, to compute moments would seemingly require doing integrals with LOTUS, but with the MGF, it is possible to find moments by taking derivatives rather than doing integrals.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
