<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Leo&#39;s sharing space</title>
        <link>http://localhost:1313/</link>
        <description>Recent content on Leo&#39;s sharing space</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 14 May 2025 15:10:48 +0800</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Beta distribution</title>
        <link>http://localhost:1313/post/beta_distribution/</link>
        <pubDate>Wed, 14 May 2025 15:10:48 +0800</pubDate>
        
        <guid>http://localhost:1313/post/beta_distribution/</guid>
        <description>&lt;img src="http://localhost:1313/post/beta_distribution/logcabin.jpg" alt="Featured image of post Beta distribution" /&gt;&lt;h2 id=&#34;beta-distribution&#34;&gt;Beta distribution
&lt;/h2&gt;&lt;p&gt;The Beta distribution is a continuous distribution on the interval (0,1). It is a generalization of the Unif(0,1) distribution.&lt;/p&gt;
&lt;p&gt;An random variable $X$ is said to have the Beta distribution with parameters $a$ and $b$, where $a,b &amp;gt; 0$, if its probability density function (PDF) is:&lt;/p&gt;
&lt;p&gt;$$f(x)=\frac{1}{\beta(a,b)}x^{a-1}(1-x)^{b-1},\quad0&amp;lt;x&amp;lt;1$$&lt;/p&gt;
&lt;p&gt;$\beta(a,b)$ is the beta function. It is a constant that chosen to make the PDF integrate to 1. We write this as $X \sim \text{Beta}(a,b)$.&lt;/p&gt;
&lt;p&gt;Taking $a=b=1$, the $Beta(1,1)$ PDF is $f(x)=1$, which is same as the uniform distribution $Unif(0,1)$, the proof is shown below.&lt;/p&gt;
&lt;h2 id=&#34;beta-function&#34;&gt;Beta function
&lt;/h2&gt;&lt;p&gt;To make the PDF of Beta distribution integrate to 1. The constant $\beta(a,b)$ is defined as:&lt;/p&gt;
&lt;p&gt;$$\beta(a,b)=\int_0^1 x^{a-1}(1-x)^{b-1}dx=\frac{(a-1)!(b-1)!}{(a+b-1)!}$$&lt;/p&gt;
&lt;p&gt;$Proof.$
Let&amp;rsquo;s use the induction to prove the above equation.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Base Case:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For $b=1$:
$$\beta(a,1)=\int_0^1x^{a-1}dx=\frac{1}{a}=\frac{(a-1)!(1-1)!}{(a+1-1)!}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Recurrence Relation of the Beta Function:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using integration by parts on $\beta(a,b)$:
$$\beta(a,b+1)=\frac{b}{a+b}\beta(a,b)$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Inductive Step:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s assume the formula holds for $b=k$, i.e., $\beta(a,k)=\frac{(a-1)!(k-1)!}{(a+k-1)!}$.&lt;/p&gt;
&lt;p&gt;Then we have:
$$\beta(a,k+1)=\int_0^1 x^{a-1}(1-x)^{k}dx=\int_0^1 x^{a-1}(1-x)^{k}dx$$
As shown in 2, we can write:
$$\beta(a,k+1)=\frac{k}{a+k}\int_0^1 x^{a-1}(1-x)^{k-1}dx$$
$$=\frac{k}{a+k}\cdot\frac{(a-1)!(k-1)!}{(a+k-1)!}$$
$$=\frac{(a-1)!(k)!}{(a+k)!}$$
Thus, we have shown that if the formula holds for $b=k$, it also holds for $b=k+1$.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the principle of mathematical induction, the formula holds for all positive integers $b$. Symmetrically, we can also show that the formula holds for $a$ by using the same induction process.&lt;/p&gt;
&lt;h2 id=&#34;beta-binomial-conjugacy&#34;&gt;Beta-Binomial conjugacy
&lt;/h2&gt;&lt;p&gt;The Beta distribution is the conjugate prior of the Binomial distribution. This means that if we have a Binomial likelihood and a Beta prior, the posterior distribution will also be a Beta distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we have a coin that we want to test for bias. We flip the coin $n$ times and observe $k$ heads. We can model this with a Binomial distribution, where the probability of heads is $p$. The likelihood function is given by:
$$P(X=k|p)=\binom{n}{k}p^k(1-p)^{n-k}$$
where $X$ is the number of heads observed in $n$ flips.
The likelihood function is a function of $p$, and it tells us how likely we are to observe $k$ heads given a particular value of $p$.&lt;/p&gt;
&lt;p&gt;For example, if we have a Binomial likelihood with parameters $n$ and $k$, and a Beta prior with parameters $a$ and $b$, the posterior distribution will be:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)=\frac{P(X=k|p)f(p)}{P(X=k)}=\frac{(_k^n)p^k(1-p)^{n-k}\cdot\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{P(X=k)}$$
$$=\frac{(_k^n)p^k(1-p)^{n-k}\cdot\frac{1}{\beta(a,b)}p^{a-1}(1-p)^{b-1}}{\int_0^1P(X=k|p)f(p)dp}$$&lt;/p&gt;
&lt;p&gt;It is difficult to calculate the denominator. Are we stucked here?&lt;/p&gt;
&lt;p&gt;Acutally, the calculation is much easier than it appears. The conditional PDF $f(p|X=k)$ is a function of $p$, which means everthing that desn&amp;rsquo;t depend on $p$ is a constant. We can drop all these constants and find the PDF up to a multiplicative constant (and then the normalizing constant is whatever it needs to make the PDF integrate to 1). We can drop the $(_k^n)$, $\frac{1}{\beta(a,b)},$ and the denominator $P(X=k)$. So we can write:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)\propto p^{k+a-1}(1-p)^{n-k+b-1}$$&lt;/p&gt;
&lt;p&gt;Which is very close to the PDF of a Beta distribution $Beta(a+k, b+n-k)$, up to a multiplicative constant. Therefore, we can conclude that the posterior distribution is:&lt;/p&gt;
&lt;p&gt;$$f(p|X=k)\sim Beta(a+k, b+n-k)$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LOTUS</title>
        <link>http://localhost:1313/post/lotus/</link>
        <pubDate>Wed, 14 May 2025 15:10:48 +0800</pubDate>
        
        <guid>http://localhost:1313/post/lotus/</guid>
        <description>&lt;img src="http://localhost:1313/post/lotus/windmill.png" alt="Featured image of post LOTUS" /&gt;&lt;h2 id=&#34;definition-of-expectation&#34;&gt;Definition of expectation
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Definition (Expectation of a discrete random variable)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The expected value of a discrete random variable $X$ whose distinct possible values are $x_1, x_2, \ldots$ is defined by
$$\mathbb E[X] = \sum_{i} x_i P(X = x_i) = \sum_{i} x_i f_X(x_i)$$
where $f_X(x)$ is the probability mass function of $X$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The expected value of a continuous random variable $X$ is defined by
$$\mathbb E[X] = \int_{-\infty}^{\infty} x f_X(x) dx$$
where $f_X(x)$ is the probability density function of $X$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;law-of-unconscious-statistician-lotus&#34;&gt;Law of unconscious statistician (LOTUS)
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Theorem (LOTUS)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If $X$ is a discrete random variable, and $g$ is a function from $\mathbb R$ to $\mathbb R$. If $Y=g(X)$, then
$$\mathbb E[Y] = \mathbb E[g(X)] = \sum_{x} g(x) P(X = x) = \sum_{x} g(x) f_X(x)$$
where $f_X(x)$ is the probability mass function (PMF) of $X$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $X$ is a continuous random variable, and $g$ is a function from $\mathbb R$ to $\mathbb R$. If $Y=g(X)$, then
$$\mathbb E[Y] = \mathbb E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) dx$$
where $f_X(x)$ is the probability density function (PDF) of $X$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;why-lotus&#34;&gt;Why LOTUS?
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X \sim Exponential(\lambda)$&lt;/li&gt;
&lt;li&gt;$Y=log(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we use the definition of expectation to calculate $\mathbb E[Y]$, we can first calculate the PDF of $Y$:
$$f_Y(y) = \frac{d}{dy} P(Y \leq y) = \frac{d}{dy} P(X \leq e^y) = \frac{d}{dy} (1 - e^{-\lambda e^y})$$
$$= \lambda e^{-\lambda e^y} e^y = \lambda e^{-\lambda e^y + y}$$&lt;/p&gt;
&lt;p&gt;Then we can calculate $\mathbb E[Y]$:
$$\mathbb E[Y] = \int_{-\infty}^{\infty} y f_Y(y) dy = \int_{-\infty}^{\infty} y \lambda e^{-\lambda e^y + y} dy$$&lt;/p&gt;
&lt;p&gt;However, LOTUS could make it easier:
$$\mathbb E[Y] = \mathbb E[log(X)] = \int_{0}^{\infty} log(x) \lambda e^{-\lambda x} dx$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X \sim N(0,1)$&lt;/li&gt;
&lt;li&gt;$Y = sin(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we use the definition of expectation to calculate $\mathbb E[Y]$, we can first calculate the PDF of $Y$:
$$f_Y(y) = \frac{d}{dy} P(Y \leq y) = \frac{d}{dy} P(X \leq sin^{-1}(y))$$
However, there’s no closed-form PDF for $Y$ as we cannot write the analytic form of $sin^{-1}(y)$. It’s a complicated, oscillatory transformation of a normal variable. You can’t write down $f_Y(y)$ analytically. Consequently, we cannot calculate $\mathbb E[Y]$ using the definition of expectation.&lt;/p&gt;
&lt;p&gt;However, we can use LOTUS:
$$\mathbb E[Y] = \mathbb E[sin(X)] = \int_{-\infty}^{\infty} sin(x) f_X(x) dx = \int_{-\infty}^{\infty} sin(x) \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$$
This is a analytically solvable integral. The result is $0$ because the integrand is an odd function and the limits are symmetric about $0$.&lt;/p&gt;
&lt;h2 id=&#34;in-summary&#34;&gt;In summary
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;LOTUS is a powerful tool for calculating the expected value of a function of a random variable.&lt;/li&gt;
&lt;li&gt;It allows us to avoid the need to find the PDF of the transformed variable.&lt;/li&gt;
&lt;li&gt;It can simplify the calculation of expected values for complex transformations.&lt;/li&gt;
&lt;li&gt;It is applicable to both discrete and continuous random variables.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Change_of_variables</title>
        <link>http://localhost:1313/post/change_of_variables/</link>
        <pubDate>Wed, 14 May 2025 14:35:45 +0800</pubDate>
        
        <guid>http://localhost:1313/post/change_of_variables/</guid>
        <description>&lt;img src="http://localhost:1313/post/change_of_variables/House.png" alt="Featured image of post Change_of_variables" /&gt;&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Change of variables in one dimension) Let $X$ be a random variable with PDF $f_X$, and let $Y=g(X)$, where $g$ is differentiable and monotonic. Then the PDF of $Y$ is given by:
$$f_Y(y)=f_X(x)|\frac{dx}{dy}|$$
where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Suppose g is strictly increasing. Then, for $y=g(x)$, the CDF of $Y$ is:
$$F_Y(y)=P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^-1(y))=F_X(g^-1(y))=F_X(x)$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, the PDF of $Y$ is
$$f_Y(y)=\frac{d}{dy}F_Y(y)=\frac{d}{dy}F_X(x)=\frac{dx}{dy}\frac{d}{dx}F_X(x)=f_X(x)\frac{dx}{dy}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Suppose g is strictly decreasing. Then, for $y=g(x)$, the CDF of $Y$ is:
$$F_Y(y)=P(Y\leq y)=P(g(X)\leq y)=P(X\geq g^-1(y))=1-P(X\leq g^-1(y))=1-F_X(g^-1(y))=1-F_X(x)$$
Therefore, the PDF of $Y$ is
$$f_Y(y)=\frac{d}{dy}F_Y(y)=-\frac{d}{dy}F_X(x)=-\frac{dx}{dy}\frac{d}{dx}F_X(x)=-f_X(x)\frac{dx}{dy}$$
In this case, the $\frac{dx}{dy}$ is negative, so we can write:
$$f_Y(y)=f_X(x)|\frac{dx}{dy}|$$&lt;/li&gt;
&lt;li&gt;Combine the two cases:
$$f_Y(y)=f_X(x)|\frac{dx}{dy}|$$
where $x=g^-1(y)$. The support of Y is all $g(x)$ with $x$ in the support of $X$.&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Taylor Expansion / Series and Power Series</title>
        <link>http://localhost:1313/post/taylor_expansion/</link>
        <pubDate>Tue, 13 May 2025 20:14:20 +0800</pubDate>
        
        <guid>http://localhost:1313/post/taylor_expansion/</guid>
        <description>&lt;img src="http://localhost:1313/post/taylor_expansion/Figure.jpg" alt="Featured image of post Taylor Expansion / Series and Power Series" /&gt;&lt;h2 id=&#34;taylor-expansion--series&#34;&gt;Taylor Expansion / Series
&lt;/h2&gt;&lt;p&gt;In mathematics, the Taylor expansion / series of a function is an infinite sum of terms that are expressed in terms of the function&amp;rsquo;s derivatives at a single point. For most common functions, the function and the sum of its Taylor series are equal near this point.&lt;/p&gt;
&lt;p&gt;Suppose $f(x)$ is a real or composite function, which is a differentiable function of a neighborhood of the point $a$. The Taylor series of $f(x)$ about the point $a$ is given by:&lt;/p&gt;
&lt;p&gt;$$f(x)=f(a)+\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2+\frac{f^{\prime\prime\prime}(a)}{3!}(x-a)^3+\dots$$&lt;/p&gt;
&lt;p&gt;Where $n!$ denotes the factorial of $n$. In the more compact sigma notation, this can be written as:
$$f(x)=\sum_{n=0}^\infin\frac{f^{(n)}(a)}{n!}(x-a)^n$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: the power series for $e^x$ is:
$$e^x=\sum_{n=0}^\infty \frac{x^n}{n!}$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As the derivative of $e^x$ is itself:
$$f^{(n)}(x)=e^x$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Taking the derivative at $x=0$:
$$f^{(n)}(0)=e^0=1$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using Taylor expansion, set $a=0$:
$$f(x)=\sum_{n=0}^\infin\frac{f^{(n)}(a)}{n!}(x-a)^n=\sum_{n=0}^\infin\frac{x^n}{n!}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taylor expansion is a way to &lt;strong&gt;approximate&lt;/strong&gt; a function using infinitely many polynomial terms.&lt;/p&gt;
&lt;h2 id=&#34;power-series&#34;&gt;Power Series
&lt;/h2&gt;&lt;p&gt;A power series is an infinite series of the form:
$$\sum_{n=0}^\infty a_n(x-a)^n$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_n$ are the coefficients of the series&lt;/li&gt;
&lt;li&gt;$x$ is the variable&lt;/li&gt;
&lt;li&gt;$a$ is a constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: the power series for $sin(x)$ is:
$$sin(x)=\sum_{n=0}^\infin\frac{(-1)^nx^{2n+1}}{(2n+1)!}=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\frac{x^9}{9!}-\frac{x^11}{11!}+\dots$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Using the Taylor expansion with $a=0$:
$$f(x)=sin(x)=\sum_{n=0}^\infin\frac{f^{(n)}(0)}{n!}(x)^n$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the derivatives of $sin(x)$ at $x=0$:
$$f(0)=sin(0)=0$$
$$f^{(1)}(0)=cos(0)=1$$
$$f^{(2)}(0)=-sin(0)=0$$
$$f^{(3)}(0)=-cos(0)=-1$$
$$f^{(4)}(0)=sin(0)=0$$
$$f^{(5)}(0)=cos(0)=1$$
$$f^{(6)}(0)=-sin(0)=0$$
$$f^{(7)}(0)=-cos(0)=-1$$
$$\dots$$
You can see that only the odd derivatives are non-zero, and they alternate in sign.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plug into the Taylor expansion:
$$f(x)=\sum_{n=0}^\infin\frac{f^{(n)}(0)}{n!}(x)^n=\sum_{n=0}^\infin\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;In Summary&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Every Taylor series is a power series, but not every power series is a Taylor series.&lt;/li&gt;
&lt;li&gt;Power series serve as a broad framework, while Taylor series are tailored (pun intended!) to approximate specific functions using their derivatives.&lt;/li&gt;
&lt;li&gt;The convergence of a Taylor series to its function depends on the function being analytic (i.e., equal to its Taylor series in some neighborhood).&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Moments in Probability</title>
        <link>http://localhost:1313/post/moments/</link>
        <pubDate>Tue, 13 May 2025 00:53:25 +0800</pubDate>
        
        <guid>http://localhost:1313/post/moments/</guid>
        <description>&lt;img src="http://localhost:1313/post/moments/Minecraft_DH.png" alt="Featured image of post Moments in Probability" /&gt;&lt;h2 id=&#34;moments-in-probability&#34;&gt;Moments in Probability
&lt;/h2&gt;&lt;p&gt;For a random variable $X$, The $n$-th moments of $X$ is the expectation of $X^n$, which is $\mathbb{E}[X^n]$.&lt;/p&gt;
&lt;p&gt;Why we need moment? Let’s starts with some measures we often use to describe a distribution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mean:&lt;/strong&gt; $\mu=\mathbb{E}[X]$. &lt;em&gt;Mean&lt;/em&gt; measures the central tendency. It tells us something about the center of a distribution. It is the $1$-th moment of $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt; $\sigma=\mathbb{E}[X-\mu]^2$, &lt;em&gt;Variance&lt;/em&gt; measures of dispersion. It is a measure of how far a set of numbers is spread out from their average value. It is the $2$-nd moment of $X-\mu$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Skewness:&lt;/strong&gt; $Skew(X)=\mathbb{E}[\frac{X-\mu}{\sigma}]^3$. &lt;em&gt;Skewness&lt;/em&gt; measures the level of asymmetry level of a distribution. It is the $3$-rd moment of standardized $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kurtosis:&lt;/strong&gt;: $Kurt(X)=\mathbb{E}[\frac{X-μ}{σ}]^4-3$. &lt;em&gt;Kurtosis&lt;/em&gt; measures the tail’s heavy level of a distribution. It is the $4$-th moment of standardized $X$. The reason for subtracting 3 is to make any Normal distribution have kurtosis of 0.&lt;/p&gt;
&lt;p&gt;As we can see from the example, moment could be used to measure the characteristics of distributions.&lt;/p&gt;
&lt;h2 id=&#34;sample-moments&#34;&gt;Sample moments
&lt;/h2&gt;&lt;p&gt;In previous section, we used some examples to introduce the moment, and why we need it. We used $\mathbb{E}[X]$, which is the $1$-st moment of $X$, to represent the mean. Back to primary school, we were taught to use $\bar{X}=\frac{1}{n}\sum_{i=1}^nx_i$ to calculate the mean of a observed data. For example, if we observe a list of $[1,3,4,2,2,6]$, the mean of this list would be $(1+3+4+2+2+6)/6=3$. Here is the question, what is the difference between $\mathbb{E}[X]$ and $\bar{X}$?&lt;/p&gt;
&lt;p&gt;In probability, we use $\mathbb{E}[X]$ to represent the mean of a random variable $X$. In statistics, we use $\bar{X}$ to represent the mean of a observed data (sampled data, also called sample). $\mathbb{E}[X]$ is the population mean of a random variable. Let&amp;rsquo;s say $X \backsim N(0,1)$, then the $\mathbb{E}[X]$ is equal to $0$. While $\bar{X}$ is the sample mean of a observed data. In the real situation, we can only observe a sample of $X$, not the whole population. Therefore, the $\bar{X}$ calculated from $\frac{1}{n}\sum_{i=1}^nx_i$ might not eactly equal to $0$. As the sample mean $\bar{X}$ is an estimate of the population mean $\mathbb{E}[X]$.&lt;/p&gt;
&lt;p&gt;Similarly, the sample variance is defined as:
$$ S_n^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2 $$
The sample variance $S_n^2$ is an estimate of the population variance $\sigma^2$. The sample variance is an unbiased estimator of the population variance. The reason for using $n-1$ instead of $n$ is to make the sample variance an unbiased estimator of the population variance. If we use $n$, then the sample variance would be biased.&lt;/p&gt;
&lt;p&gt;Similarly, we can define the &lt;em&gt;sample skewness&lt;/em&gt; as:
$$ S_n^3=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar{x}}{S_n})^3 $$
and the &lt;em&gt;sample kurtosis&lt;/em&gt; as:
$$ S_n^4=\frac{1}{n}\sum_{i=1}^n(\frac{x_i-\bar{x}}{S_n})^4-3 $$&lt;/p&gt;
&lt;h2 id=&#34;moment-generating-functions&#34;&gt;Moment generating functions
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (Moment via derivatives of the MGF). The moment generating function (MGF) of a random variable $X$ is defined as:
$$ M_X(t)=\mathbb{E}[e^{tX}] $$
Given the MGF of $X$, we can get the $n$-th moment of $X$ by taking the $n$-th derivative of the MGF and evaluating it at $t=0$: $$\mathbb{E}[X^n]=M^{(n)}(0)$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Using Taylor expansion of $M_X(t)$ around $t=0$:
$$M_X(t)=\mathbb{E}[e^{tX}]=\sum_{n=0}^\infin M^{(0)}(0)\frac{t^n}{n!} \tag{1}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using Power series expansion of $e^{tX}$:
$$e^{tX}=\sum_{n=0}^\infin \frac{(tX)^n}{n!}=\sum_{n=0}^\infin \frac{t^nX^n}{n!}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So the expectation of $e^{tX}$ is:
$$\mathbb{E}[e^{tX}]=\sum_{n=0}^\infin \frac{t^n}{n!}\mathbb{E}[X^n] \tag{2}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Comparing (1) and (2), we can get:
$$\mathbb{E}[X^n]=M^{(n)}(0)$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This theorem is suprising in that for a continuous random variable $X$, to compute moments would seemingly require doing integrals with LOTUS, but with the MGF, it is possible to find moments by taking derivatives rather than doing integrals.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>http://localhost:1313/page/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/page/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>About</title>
        <link>http://localhost:1313/page/about/</link>
        <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/page/about/</guid>
        <description>&lt;p&gt;Written in Go, Hugo is an open source static site generator available under the &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio/hugo/blob/master/LICENSE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Apache License 2.0.&lt;/a&gt; Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.&lt;/p&gt;
&lt;p&gt;Hugo makes use of a variety of open source projects including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/yuin/goldmark&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/yuin/goldmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/alecthomas/chroma&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/alecthomas/chroma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/muesli/smartcrop&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/muesli/smartcrop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/cobra&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/cobra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/viper&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/viper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.&lt;/p&gt;
&lt;p&gt;Hugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.&lt;/p&gt;
&lt;p&gt;Websites built with Hugo are extremely fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.&lt;/p&gt;
&lt;p&gt;Learn more and contribute on &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Links</title>
        <link>http://localhost:1313/page/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/page/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;links&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;GitHub&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;website&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://github.com&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TypeScript&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;website&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ts-logo-128.jpg&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>http://localhost:1313/page/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/page/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
